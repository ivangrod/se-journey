[
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "AWS  AWS - EC2   AWS - Security   AWS - Ops   AWS - Infrastructure   "
},
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/aws/ec2/",
	"title": "AWS - EC2",
	"tags": [],
	"description": "",
	"content": " Launch virtual machines on the cloud Storing data on virtual devices (EBS) Distributing load across machines (ELB) Scaling the services using an auto-scaling group (ASG)   AMI: Amazon Machine Image. An image to use to create our instances. They\u0026rsquo;re built for a specific AWS region.\n EC2 User Data AWS comnes with base images which can be customized using EC2 User Data:\nBootstrapping in an EC2 instance with EC2 user data script. Only run once \u0026lt;-\u0026gt; First start.\nIt\u0026rsquo;s used to automated boot tasks:\n Installing updates Installing software Downloading common files  EC2 Instance Launch Types  On Demand: Short workloads. Pay for what you use.  Recommend for short-term and un-interrupted workloads, where you can\u0026rsquo;t predict how the application will behave.\n Reserved: Long workloads (\u0026gt;= 1 year). Pay upfront for what you use with long term commitment  Recommend if you have a database and you know it\u0026rsquo;s going to be steady for a year or three years\n  Convertible reserved: Long workloads with flexible instances. Insecure about needing a C4, C4 large\n  Scheduled reserved: Reserve capacity, but only for a small window.\n  Spot: Short workloads. Very cheap. Loose instances.\n  Recommend for batch jobs, Big data analysis or workloads that are resilient to failures.\n Dedicated: No other customer will share our underlying hardware. Visibility into the underlying sockets / physical cores of the hardware. Dedicated Hosts: Book the entire physical server  https://www.ec2instances.info/ There are five distints characteristics:\n RAM (type, amount, generation) CPU (type, frequency, generation, number of cores) I/O (disk performance, ESB optimisations) Network (network bandwidth, network latency) GPU (Graphical Processing Unit)   T2/T3 are burstable instances. Can be amazing to handl unexpected traffic and getting the insurance that it will be handled correctly.\n "
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/tdd/tdd-schools/",
	"title": "TDD Schools",
	"tags": [],
	"description": "",
	"content": "Inside-out (Classic school) - Algorithms Enfocada en la verificación del estado de los objetos, siendo por ello imprescindible que el contexto de los test siempre deba estar formado por \u0026ldquo;objetos reales\u0026rdquo;, configurados previamente.\nPatterns: Mother Objects, Factory\nApproach:\nWe generalise the solution one failing test at a time so we end up with a simple, elegant solution that satisfies all of the tests up to that point. Based on triangulation\nOutside-in (London school) - Interactions Enfocada en verificar que el comportamiento de los objetos es el esperado. Verificar las correctas interacciones entre objetos, y no el estado en sí mismo de los objetos.\nFocusing on roles, responsibilities and interactions, as opposed to algorithms. It\u0026rsquo;s born out of the furnace of component-based, distributed and service-oriented applications\nPatterns: Test Double\nApproach:\nIdentify the roles, responsibilities and key interactions/collaborations between roles in an end-to-end implementation of the solution to satisfy a system-level scenario or acceptance test. Implement the code needed in each collaborator, one at a time, faking it\u0026rsquo;s direct collaborators and then work your way down through the \u0026ldquo;call stack\u0026rdquo; of interactions until you have a working end-to-end implementation that passes the front-end test.\nDemo Outside-in TDD https://www.youtube.com/watch?v=XHnuMjah6ps\nhttps://www.youtube.com/watch?v=gs0rqDdz3ko\nhttps://www.youtube.com/watch?v=R9OAt9AOrzI\nNotes\n  Use verify().methodXXX -\u0026gt; Test over commands method (void)\n  For command methods you need to test the side effects. After you identify the trigger for them.\n  You can\u0026rsquo;t test what you can\u0026rsquo;t control it\n  class Clock{ public String todayAsString(){ LocalDate today = today(); return today.format(DateTimeFormatter.ofPattern(\u0026#34;dd/MM/yyyy\u0026#34;)); } // Extract the randomness  protected LocalDate today(){ return LocalDate.now(); } } class ClockShould{ @Test public void return_todays_date_in_dd_MM_yyyy_format(){ Clock clock = new TesteableClock(); ... } private class TesteableClock extends Clock{ // Override and control the behaviour of the randomness  @Override protected LocalDate today(){ return LocalDate.of(2020,3,04); } } } "
},
{
	"uri": "https://ivangrod.github.io/se-journey/agile/",
	"title": "Agile",
	"tags": [],
	"description": "",
	"content": "Agile "
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/tdd/",
	"title": "Test-Driven Development",
	"tags": [],
	"description": "",
	"content": "Steps   Quickly add a test.\n  Run all tests and see the new one fail.\n  Make a little change.\n  Run all tests and see them all succeed.\n  Refactor to remove duplication.\nThe problem is the dependency between the code and the test—you can\u0026rsquo;t change one without changing the other. Our goal is to be able to write another test that “makes sense” to us, without having to change the code.\nIf dependency is the problem, duplication is the symptom.\nBy eliminating duplication before we go on to the next test, we maximize our chance of being able to get the next test running with one and only one change.\n  Points of interest  How each test can cover a small increment of functionality How small and ugly the changes can be to make the new tests run How often the tests are run How many teensy-weensy steps make up the refactorings  Beginning  We\u0026rsquo;ll make a to-do list to remind us what we need to do, to keep us focused and to tell us when we are finished.   We don\u0026rsquo;t start with objects, we start with tests.\n   Told a story with a snippet of code about how we wanted to view one operation\n  Ignored the details of JUnit for the moment\n  Made the test compile with stubs\n  Made the test run by committing horrible sins\n  Gradually generalized the working code, replacing constants with variables\n  Added items to our to-do list rather than addressing them all at once\n  TDD Patterns Three Laws of TDD   Law 1: You can’t write any production code until you have first written a failing spec.\n  Law 2: You can’t write more of a unit test than is sufficient to fail, and not compiling is failing.\n  Law 3: You can’t write more production code than is sufficient to pass the currently failing unit test.\n  Rule of Three Rule of three (\u0026ldquo;Three strikes and you refactor\u0026rdquo;) is a code refactoring rule of thumb to decide when similar pieces of code should be refactored to avoid duplication. It states that two instances of similar code don\u0026rsquo;t require refactoring, but when similar code is used three times, it should be extracted into a new procedure.\nTest-Driven Development Patterns TEST (NOUN) Positive feedback loop. The more stress you feel, the less testing you will do. The less testing you do, the more errors you will make. The more errors you make, the more stress you feel.\nISOLATED TEST  Make the tests so fast to run that you can run them myself, and run them often. That way you can catch errors before anyone else sees them\nA huge stack of errors didn\u0026rsquo;t usually mean a huge list of problems\n One convenient implication of isolated tests is that the tests are order independent.\nA second implication of isolated tests is that you have to work, sometimes work hard, to break your problem into little orthogonal dimensions\nTEST LIST TEST FIRST ASSERT FIRST  \u0026ldquo;What is the right answer?\u0026rdquo; || \u0026ldquo;How am I going to check?\u0026rdquo;\n TEST DATA  Use data that makes the tests easy to read and follow.\nDon\u0026rsquo;t have a list of ten items as the input data if a list of three items will lead you to the same design and implementation decisions.\n EVIDENT DATA Include expected and actual results in the test itself, and try to make their relationship apparent.\nRed Bar Patterns ONE STEP TEST Pick a test from the list that will teach you something and that you are confident you can implement. Each test should represent one step toward your overall goal.\nSTARTER TEST Start by testing a variant of an operation that doesn\u0026rsquo;t do anything.\n The first question you have to ask with a new operation is, \u0026ldquo;Where does it belong?\u0026rdquo; Until you\u0026rsquo;ve answered this question, you won\u0026rsquo;t know what to type for the test.\n If you write a realistic test first, then you will find yourself solving a bunch of problems at once. Beginning with a realistic test will leave you too long without feedback.\nEXPLANATION TEST Ask for and give explanations in terms of tests.\nA companion technique is to start giving explanations in terms of tests: \u0026ldquo;Here\u0026rsquo;s how it works now. When I have a Foo like this and a Bar like that, then the answer is 76. If I have a Foo like that and a Bar like this, though, I would like the answer to be 67.\u0026rdquo;\nLEARNING TEST When do you write tests for externally produced software? Before the first time you are going to use a new facility in the package.\nEx.: A project in which Learning Tests were routinely written. When new releases of the package arrived, first the tests were run (and fixed, if necessary). If the tests didn\u0026rsquo;t run, then there was no sense running the application because it certainly wouldn\u0026rsquo;t run.\nANOTHER TEST How do you keep a technical discussion from straying off topic? When a tangential idea arises, add a test to the list and go back to the topic.\nREGRESSION TEST When a defect is reported write the smallest possible test that fails and that, once run, will be repaired.\nRegression tests for the application give your users a chance to speak concretely to you about what is wrong and what they expect.\nBREAK  If you know what to type, type the Obvious Implementation. If you don\u0026rsquo;t know what to type, then Fake It. If the right design still isn\u0026rsquo;t clear, then Triangulate. If you still don\u0026rsquo;t know what to type, then you can take that shower.\n Testing Patterns CHILD TEST Write a smaller test case that represents the broken part of the bigger test case. Get the smaller test case running. Reintroduce the larger test case.\nMOCK OBJECT When you have an object that relies on an expensive or complicated resource, you must create a fake version of the resource that answers constants.\nSELF SHUNT For testing that one object communicates correctly with another, the best way is having the object under test communicate with the test case instead of with the object it expects.\n// TODO Review it\nLOG STRING Test that the sequence in which messages are called is correct -\u0026gt; Keep a log in a string, and append to the string when a message is called.\nLog Strings are particularly useful when you are implementing Observer and you expect notifications to come in a certain order.\nCRASH TEST DUMMY Test error code that is unlikely to be invoked -\u0026gt; Invoke it anyway with a special object that throws an exception instead of doing real work. Code that isn\u0026rsquo;t tested doesn\u0026rsquo;t work.\nprivate class FullFile extends File { public FullFile(String path) { super(path); } public boolean createNewFile() throws IOException { throw new IOException(); } } public void testFileSystemError() { File f = new FullFile(\u0026#34;foo\u0026#34;); try { saveAs(f); fail(); } catch (IOException e) {} } A Crash Test Dummy is like a Mock Object, except you don\u0026rsquo;t need to mock up the whole object. Java\u0026rsquo;s anonymous inner classes work well for sabotaging just the right method to simulate the error we want to exercise:\npublic void testFileSystemError() { File f= new File(\u0026#34;foo\u0026#34;) { public boolean createNewFile() throws IOException { throw new IOException(); } }; try { saveAs(f); fail(); } catch (IOException e) {} } BROKEN TEST Finish a solo session by writing a test case and running it to be sure it doesn\u0026rsquo;t pass. When you come back to the code, you then have an obvious place to start.\nYou have an obvious, concrete bookmark to help you remember what you were thinking.\nCLEAN CHECK-IN How do you leave a programming session when you\u0026rsquo;re programming in a team? Leave all of the tests running.\nYou need to start from a place of confidence and certainty.\nGreen Bar Patterns FAKE IT (\u0026lsquo;TIL YOU MAKE IT) Return a constant and gradually replace constants with variables until you have the real code. Remove duplication between the test and the production code.\n  Having a green bar feels completely different from having a red bar. When the bar is green, you know where you stand. You can refactor from there with confidence.\n  Starting with one concrete example and generalizing from there prevents you from prematurely confusing yourself with extraneous concerns.\n  TRIANGULATION We only generalize code (abstract) when we have two examples or more.\nWe briefly ignore the duplication between test and model code. When the second example demands a more general solution, then and only then do we generalize.\nOnce we have the two assertions and we have abstracted the correct implementation for plus, we can delete one of the assertions on the grounds that it is completely redundant with the other.\nOBVIOUS IMPLEMENTATION Type in the real implementation.\n Advice Commonly shift between two modes of implementation. When everything is going smoothly and you know what to type, you put in Obvious Implementation after Obvious Implementation (running the tests each time to ensure that what\u0026rsquo;s obvious to yours is still obvious to the computer). As soon as you get an unexpected red bar, you back up, shift to Faking implementations, and refactor to the right code. When you confidence returns, you go back to Obvious Implementations.\n ONE TO MANY An operation that works with collections of objects -\u0026gt; Implement it without the collections first, then make it work with collections.\nTPP - TRANSFORMATION PRIORITY PREMISE Cada nuevo test que convertimos a verde debe provocar una transformación en el código de producción que lo haga un poco más genérico de lo que era antes de añadir ese test. Secuencia de transformaciones propuesta por Robert C. Martin:\n {} –\u0026gt; nil: De no haber código a devolver nulo. nil -\u0026gt; constant: De nulo a devolver un valor literal. constant -\u0026gt; constant+: De un valor literal simple a uno más complejo. constant -\u0026gt; scalar: De un valor literal a una variable. statement -\u0026gt; statements: Añadir más líneas de código sin condicionales. unconditional -\u0026gt; if: Introducir un condicional scalar -\u0026gt; array: De variable simple a colección. array -\u0026gt; container: De colección a contenedor. statement -\u0026gt; recursion: Introducir recursión. if -\u0026gt; while: Convertir condicional en bucle. expression -\u0026gt; function: Reemplazar expresión con llamada a función. variable -\u0026gt; assignment: Mutar el valor de una variable.  xUnit Patterns ASSERTION Write boolean expressions that automate your judgment about whether the code worked.\n Si para validar un único comportamiento necesitamos varias líneas de asserts, a veces es mejor crear un método propio. Otra opción es implementar Custom Matchers\n Optional first parameter -\u0026gt; Add information about the assertion that will be printed if it ever fails. Some teams adopt the convention that all assertions must be accompanied by an informative error message.\nFIXTURE Cada test debería ser autosuficiente para crear el conjunto de datos que necesita. Estos datos son los fixtures.\nHow do you create common objects needed by several tests -\u0026gt; Convert the local variables in the tests into instance variables. Override setUp() and initialize those variables.\n  Test-fixture-creating code with the test -\u0026gt; Tests written with the setup code right there with the assertions are readable from top to bottom.\n  Test-fixture-creating code into a method called setUp() -\u0026gt; Set instance variables to the objects that will be used in the test. We would have to remember that the method was called, and remember what the objects looked like, before we could write the rest of the test.\n   In general, if I find myself wanting a slightly different fixture, then I start a new subclass of TestCase. @Before annotation may help us to evaluate the test class context.\n Fixture Setup Patterns\nEXTERNAL FIXTURE How do you release external resources in the fixture? Override tearDown() and release the resources.\nTEST METHOD Los nombres de los test deberían seguir siendo válidos cuando los pequeños detalles de implementación cambien. No deberían cambiar mientras que las reglas de negocio no cambien\nTip: Usar snake_case debido a que los nombres de los test tienden a ser muy largos. Al ser métodos que no van a ser invocados desde ninguna otra parte del código productivo, no supone problema que rompan la convención del lenguaje que se esté utilizando.\nEXCEPTION TEST How do you test for expected exceptions? Catch expected exceptions and ignore them, failing only if the exception isn\u0026rsquo;t thrown.\nTesting Exceptions (GitHub)\npublic void testMissingRate() { try { exchange.findRate(\u0026#34;USD\u0026#34;, \u0026#34;GBP\u0026#34;); fail(); /* Notice that we are careful only to catch the particular exception we expect, so we will also be notified if the wrong kind of exception is thrown */ } catch (IllegalArgumentException expected) {} } ALL TESTS How do you run all tests together? Make a suite of all the suites—one for each package, and one aggregating the package tests for the whole application.\nBest Practice Git   Para descartar los cambios locales en el paso green -\u0026gt; refactor basta con un git reset para volver al punto en el que los test pasan.\n  Commit en pequeños cambios incrementales. Para que nos formen parte del historial de Git, por algún motivo o por política de equipo, pueden unificarse con git squash\n  // TODO ¿Guía TDD con VCS?\nLegacy code Al no tener garantías de que el sistema se comporte como uno espera que lo haga, puede ser que plantear test partiendo de premisas incorrectas sea una pérdida de tiempo. Proceso para abordar ésto:\n Guardar cualquier cambio pendiente que tuvieras hasta ese momento (git commit) Añadir la nueva funcionalidad Ejecutar la aplicación a mano para validar que funciona como se espera Explorar la aplicación como tester/usuario para asegurar que ninguna otra funcionalidad está rota Añadir test automáticos que den cobertura a la nueva funcionalidad Verlos ejecutarse en verde Volver a dejar el código como estaba al principio (volviendo al commit anterior si es necesario) Lanzar de nuevo los test y verlos en rojo, confirmando que el error es el esperado Recuperar la última versión del código para ver los test en verde de nuevo  Exceptions Utilizar assert también para excepciones \u0026gt; código simétrico suele ser más fácil de entender :) :\n@Test public void should_fail_if_the_file_is_empty(){ assertThatExceptionOfType(IllegalFileException.class) .isThrownBy(() -\u0026gt; { filter.apply(emptyFile); }) } Sentencia declarativa: La expresión se evalúa de dentro hacia fuera. La función isThownBy nunca llegará a ejecutarse, porque el test se detendría antes con un rojo.\nTesting Exceptions (GitHub)\nPitfalls  Infravalorar el nombre de los test: Mayor entendimiento del problema y de la solución. Simplificar. Documentación viva y expresiva. Testar estructuras y asignaciones: Test demasiado acoplados a la implementación del código sin necesidad. No ayudan a implementar ninguna funcionalidad. Falta de refactoring en test: En un primer momento, no importa si el test tiene diez líneas. Cuando esté en verde, podremos mejorar la legilibilidad del test. Mocks return mocks: Tests que entorpecen, encarecen el mantenimiento. Pueden ser test de usar y tirar. Uso de variables estáticas/compartidas: Es recomendable lanzar baterías con las diferentes suites de test en paralelo en máquinasde varios núcleos. Ignorar test en rojo: No dejarlos ignorados más de un/dos días. Más de una regla por test: El test debe poner de manifiesto sólo una de las reglas de negocio. Cuando falle, tendremos mejor entendimiento de las consecuencias que puede acarrear. Introducir complejidad ciclomática: Cualquier cambio que añada indirección o cualquier otra posible complejidad en los test debe hacerse en la fase de refactor. Test parametrizados: Pocas veces es útil, ya que usando triangulación con dos o tres casos podríamos obtener el mismo resultado. Si fuera necesario, utilizar herramientas de test basados en propiedades. Forzar el diseño para poder probar: La interfaz publica de un módulo o de una clase es un compromiso adquirido con sus consumidores. Esperas aleatorias para resolver asincronía: Para que los test inspiren confianza tienen que ser deterministas y además rápidos en la ejecución. Dependencias de plataforma y de máquina. Ausencia de exploratory testing: Probar funcionalidades que no hemos probado nosotros. Exceso de test de la GUI: Tests que atacan a la interfaz gráfica son los más frágiles de todos. Cadenas de transiciones entre estados: Partir el test en varios, de forma que cada uno se limita a verificar una sola transición de estado. Quizás necesitemos alguna vía de configuración de partida. Ausencia de documentación.  "
},
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/aws/security/",
	"title": "AWS - Security",
	"tags": [],
	"description": "",
	"content": "IAM - Identity and Access Management. Global view (non-region)\nUser -\u0026gt; Usually a physical person Group -\u0026gt; Functions (admin, devops) / Teams (engineering, design) Role -\u0026gt; Internal usage within AWS resources\nPolicies -\u0026gt; Defines what each of the above can or can\u0026rsquo;t do. Permissions are governed by Policies. + Least Privilege principle\nSecurity Groups Network security in AWS. They control how traffic will be allowed into your EC2 Machines.\n  Access to ports\n  Authorised IP ranges - IPv4 and IPv6\n  Control of inbound network\n  Control of outbound network\n  Can be attached to multiple instances\n  Locked down to a region/VPC (Virtual Private Cloud) combination\n   It\u0026rsquo;s good to maintain one separate security group for SSH access\n Elastic IP  Stop -\u0026gt; Start an instance EC2 -\u0026gt; Change the public IP Elastic IP -\u0026gt; Fixed public IPv4   Avoid Elastic IP. Often reflect poor architectural decisions It\u0026rsquo;s better use a random public IP and register a DNS name to it It\u0026rsquo;s better use a Load Balancer and don\u0026rsquo;t use a public IP\n "
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/test-types/",
	"title": "Test Types",
	"tags": [],
	"description": "",
	"content": " Claros, concisos y certeros: Arrange - Act - Assert / Given - When - Then. Datos mínimos relevantes para entender y distinguir cada escenario. Feedback rápido e informativo Simplicidad: Dependencias - librerías, frameworks - ¿cuánto cuesta entender como funcionan todas estas piezas? Robustez: Cuando un test se rompe debería hacerlo por un solo motivo. Debería ser muy expresivo, señalando el motivo del fallo como para que no haga falta depurar el sistema. Flexibilidad: Refactoring Isolation: El resultado de un test no debería estar sujeto a la ejecución de otro test, deberían ser independientes a la hora de ejecutarse.  Unit Integration End to end  Comparación con unit test  + granularidad - velocidad + complejidad + fragilidad - acoplamiento a la implementación del sistema + requerimiento de infraestructura para poder ser inocuo + dependencia del entorno de ejecución del sistema    Mutation Consiste en introducir pequeños cambios en el código de producción a modo de defectos, por ejemplo, invertir una condición lógica, y forzar así un fallo en los tests.\nExploratory Explorar la aplicación para descubrir cómo romperla\n Pair testing: Explorar el software a pares. Si una de las personas tiene más habilidad explorando y la otra escribiendo test automáticos, pueden aprovechar para ir automatizando las pruebas que están sin cubrir. Mob testing: Explorar el software en grupo.  Property based Ejecutar miles de casos que ponen a prueba el código de forma que un humano escribindo test automáticos no haría.\n// TODO Karumi -\u0026gt; MaxibonKata -\u0026gt; Java y JUnit-QuickCheck\nSnapshot Testing pyramid  Few e2e tests. A lot unit tests\n "
},
{
	"uri": "https://ivangrod.github.io/se-journey/culture/",
	"title": "Culture",
	"tags": [],
	"description": "",
	"content": "Culture "
},
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/aws/ops/",
	"title": "AWS - Ops",
	"tags": [],
	"description": "",
	"content": "Access Access: Configure Instance details \u0026gt; Advance details \u0026gt; User data\nAccess: SSH / EC2 Instace connect\nchmod 0400 udemy-instance-key.pem (restrict private key) SSH -\u0026gt; ssh -i udemy-instance-key.pem ec2-user@34.240.37.21\n"
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/double-patterns/",
	"title": "Double Patterns",
	"tags": [],
	"description": "",
	"content": "http://xunitpatterns.com/Test%20Double%20Patterns.html\n"
},
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/aws/infrastructure/",
	"title": "AWS - Infrastructure",
	"tags": [],
	"description": "",
	"content": "AWS Regions Availability Zone (AZ) (us-east-1a, us-east-2)\nPhysical data center in the region. Isolated from others from disasters.\n"
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/testing-resources/",
	"title": "Testing Resources",
	"tags": [],
	"description": "",
	"content": "xUnit  xUnit Patterns  BDD  Example mapping  "
},
{
	"uri": "https://ivangrod.github.io/se-journey/testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Testing "
},
{
	"uri": "https://ivangrod.github.io/se-journey/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": "Cloud "
},
{
	"uri": "https://ivangrod.github.io/se-journey/git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "Git "
},
{
	"uri": "https://ivangrod.github.io/se-journey/templates/",
	"title": "Templates",
	"tags": [],
	"description": "",
	"content": "Templates "
},
{
	"uri": "https://ivangrod.github.io/se-journey/glossary/",
	"title": "Glossary",
	"tags": [],
	"description": "",
	"content": "Glossary "
},
{
	"uri": "https://ivangrod.github.io/se-journey/agile/user-story/",
	"title": "User Story",
	"tags": [],
	"description": "",
	"content": "   How to split user stories   Story-Splitting-Flowchart.pdf  (744 ko)    "
},
{
	"uri": "https://ivangrod.github.io/se-journey/glossary/terms/",
	"title": "Terms",
	"tags": [],
	"description": "",
	"content": " ACL -\u0026gt; Access Control Lists are network traffic filters that can control incoming or outgoing traffic. FIRST -\u0026gt; Testing context: Fast, Independent, Repeatable, Self-validated, Timely. YAGNI -\u0026gt; You ain\u0026rsquo;t gonna need it  "
},
{
	"uri": "https://ivangrod.github.io/se-journey/",
	"title": "Software Engineer Journey",
	"tags": [],
	"description": "",
	"content": "Software Engineer Journey  Agile   Culture   Testing   Cloud   Git   Templates   Glossary   "
},
{
	"uri": "https://ivangrod.github.io/se-journey/templates/issue/",
	"title": "Issue",
	"tags": [],
	"description": "",
	"content": "I\u0026rsquo;m submitting a \u0026hellip;  bug report feature request support request  What is the current behavior? …\nIf the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem via https://plnkr.co or similar (you can use this template as a starting point: http://plnkr.co/edit/tpl:AvJOMERrnz94ekVua0u5).\nAny logs, error output, etc? (If it’s long, please paste to https://ghostbin.com/ and insert the link here.)\nWhat are the steps to reproduce this issue?  … … …  What is the expected behavior? …\nWhat is the motivation / use case for changing the behavior? Please tell us about your environment:  Version: 2.0.0-beta.X Browser: [all | Chrome XX | Firefox XX | IE XX | Safari XX | Mobile Chrome XX | Android X.X Web Browser | iOS XX Safari | iOS XX UIWebView | iOS XX WKWebView ] Operating System: Language: [all | TypeScript X.X | ES6/7 | ES5 | Dart] SDK Version  Other information Detailed explanation, stacktraces, related issues, suggestions how to fix, links for us to have context (eg. stackoverflow, gitter, etc)\n"
},
{
	"uri": "https://ivangrod.github.io/se-journey/templates/pull-request/",
	"title": "Pull Request",
	"tags": [],
	"description": "",
	"content": "Checklist\nPlease check if your PR fulfills the following requirements:\n Tests for the changes have been added (for bug fixes / features) Docs have been reviewed and added / updated if needed (for bug fixes / features) Build was run locally and any changes were pushed Lint has passed locally and any fixes were made for failures  Pull request type Please check the type of change your PR introduces:\n Bugfix Feature Code style update (formatting, renaming) Refactoring (no functional changes, no api changes) Build related changes Documentation content changes Other (please describe):  What is the current behavior? Issue Number: N/A\nWhat is the new behavior?  \u0026hellip; \u0026hellip; \u0026hellip;  Does this introduce a breaking change?  Yes No  If this introduces a breaking change, please describe the impact and migration path for existing applications below.\nOther information Any other information that is important to this PR such as screenshots of how the component looks before and after the change.\n"
},
{
	"uri": "https://ivangrod.github.io/se-journey/git/git-commit-message/",
	"title": "Git Commit Message",
	"tags": [],
	"description": "",
	"content": "Capitalized, short (50 chars or less) summary More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. In some contexts, the first line is treated as the subject of an email and the rest of the text as the body. The blank line separating the summary from the body is critical (unless you omit the body entirely); tools like rebase can get confused if you run the two together. Write your commit message in the imperative: \u0026quot;Fix bug\u0026quot; and not \u0026quot;Fixed bug\u0026quot; or \u0026quot;Fixes bug.\u0026quot; This convention matches up with commit messages generated by commands like git merge and git revert. Further paragraphs come after blank lines. - Bullet points are okay, too - Typically a hyphen or asterisk is used for the bullet, followed by a single space, with blank lines in between, but conventions vary here - Use a hanging indent  git log \u0026ndash;pretty=oneline shows a terse history mapping containing the commit id and the summary git rebase \u0026ndash;interactive provides the summary for each commit in the editor it invokes If the config option merge.summary is set, the summaries from all merged commits will make their way into the merge commit message git shortlog uses summary lines in the changelog-like output it produces git format-patch, git send-email, and related tools use it as the subject for emails reflogs, a local history accessible with git reflog intended to help you recover from stupid mistakes, get a copy of the summary gitk has a column for the summary GitHub uses the summary in various places in their user interface  "
},
{
	"uri": "https://ivangrod.github.io/se-journey/culture/code-review/",
	"title": "Code Review",
	"tags": [],
	"description": "",
	"content": " Code review is a human communication process\n 3-steps process   The developer who wrote the code creates a pull request (PR) and lets the team know that the PR was created. (Slack bot)\n  Once the PR is created, someone else starts reviewing the code, in accordance with a guide providing rules, standards and processes.\n  After one or two developers have reviewed the code, one or both steps should be followed:\n The code needs improvement. Then, the developer could either start a discussion regarding the best way to solve the problem, or just follow the suggestions. This step is repeated until all developers are involved and agree on the fact that the code is good enough. The iterations stop and the code can be merged back into the development branch. If the code is good enough it can be merged into the development branch.    Guide For An Effective Code Review Code style guide (Severity = Low)  In may be used to limit method body length, cyclomatic complexity, usage of legacy methods and much more. All this while requiring the format of the code to be common.\nLinting and Code Style Check Leave static code analysis and coding style check to machines with tools like SonarQube and ESLint, and spare human eyes for important parts like business logics and algorithms.\nGoogle Java Style Guide\n https://github.com/google/google-java-format  Software Architecture (Severity = Medium)  Software architecture is the process of converting software characteristics such as flexibility, scalability, feasibility, reusability, and security into a structured solution that meets the technical and the business expectations.\nSoftware Design (SOLID) (Severity = Medium) Change Traceability  It is assumed that changes to the code are done based on specifications. These specs should be re-iterated very briefly in:\n Change log. // TODO Git commit messages. The user performing the changes must also be identifiable.   Git history shall be kept clean by properly rebasing feature branches before merging them back into the main development branch. // TODO Rebase feature branches.\n Documentation Coverage (Severity = Medium)  Documentation of the code is not to be a separate process, but rather be done together with the implementation.\n Focus also on the “why” and “how” not only on the “what”.\n Provide enough context for creating meaningful pull request  Issue template Pull request template  New Developer On-boarding  The review process provides an opportunity for mentorship and collaboration, and, minimally, diversifies the understanding of code in the code base.\nDependencies (Severity = Medium)  The security, licensing and versioning implications of 3rd party and open source solutions must be considered and monitored.\nAntipatterns (Severity = High) Maintainability (Severity = High)   Extending Testing Debugging Configuring Deploying Automating  May be scored based on the efforts needed for the application\nExtensibility (Severity = Medium) Legibility (Severity = High)  Legible code is more reusable, bug-free, and future-proof\nReusability (Severity = High)  Components used across the application(s) should be identified and modularised for better reusability and maintainability.\nSecurity (Severity = High)    OWASP Secure Code Review Guide   OWASP_Code_Review_Guide_v2.pdf  (2368 ko)    Performance (Severity = High)   Asynchronous Parallel execution Caching Appropriate usage of resources Memory leaks  Scaleability (Severity = High) Usability (Severity = Medium)  The interfaces for other services, applications or 3rd parties shall be well documented and follow standards in order to be usable.\nThe APIs should be implementable and testable solely using the documentation available without any dedicated support personnel.\nTesting Coverage (Severity = High) Automation Of Builds And Deployments Motivation   Committers are motivated Sharing knowledge  Scope and size (Severity = Medium)  Changes should have a narrow, well-defined, self-contained scope that they cover exhaustively.\nKeep changes small Treat each PR as a releasable unit (feature, bug fix) or a cohesive idea which is meaningful to the PR.\n Single-piece flow -\u0026gt; LEAN manufacturing\n If a CR makes substantive changes to more than ~5 files, or took longer than 1–2 days to write, or would take more than 20 minutes to review, consider splitting it into multiple self-contained CRs.\nReview often and shorten sessions Code reviews in reasonable quantity, at a slower pace, for a limited amount of time result in the most effective code review.\n 3PM code review rule -\u0026gt; Every day at 3PM is Code Review time. Improving burndown charts.\n Refactoring changes   Refactoring changes should not alter behavior\nA behavior-changing changes should avoid refactoring and code formatting changes.\n  Refactoring changes often touch many lines and files and will consequently be reviewed with less attention. Large refactoring changes break cherry-picking, rebasing, and other source control magic. Expensive human review time should be spent on the program logic rather than style, syntax, or formatting debates.  Measure the progress   If you can’t see it, you can’t measure it, you can’t improve it!\n  SonarQube project dashboard Team based code coverage PR Comment Notification PR size and resolution time. Making a releasable task/change small is a valid skill in DevOps. We try to promote this idea with the Resolution time vs. PR size chart  Checklist for Developers Implementation\n My code compiles I have considered proper use of exceptions I have made appropriate use of logging I have eliminated unused imports I have eliminated IDE warnings I have considered possible NPEs Was performance considered? Was security considered? Does the code release resources? (HTTP connections, DB connection, files, etc) Can any code be replaced by calls to external reusable components or library functions? Thread safety and possible deadlocks  Legibility and style\n My code is tidy (indentation, line length, no commented-out code, no spelling mistakes, etc) The code follows the coding standards  Maintainability\n My code includes javadoc where appropriate My code has been developer-tested and includes unit tests Are there any leftover stubs or test routines in the code? Are there any hardcoded, development only things still in the code? Corner cases well documented or any workaround for a known limitation of the frameworks  Checklist for Reviewers Purpose\n Does this code accomplish the author’s purpose? All functions and classes exist for a reason.  Implementation\n The functionality fits the current design/architecture Types have been generalized where possible Parameterized types have been used appropriately Frameworks have been used appropriately Does the change follow standard patterns? Do you see potential for useful abstractions? Command classes have been designed to undertake one task only The code does not use unjustifiable static methods/blocks Logging used appropriately (proper logging level and details) NPEs and AIOBs Exceptions have been used appropriately Does the change add compile-time or run-time dependencies (especially between sub-projects)? Potential threading issues have been eliminated where possible Any security concerns have been addressed Performance was considered  Legibility and style\n Repetitive code has been factored out The code complies to coding guidelines and code style Does this code have TODOs?. Have the author submit a ticket on GitHub Issues or JIRA and attach the issue number to the TODO.  Maintainability\n Comments are comprehensible Comments are neither too numerous nor verbose Unit tests are present and they covering interesting cases Does this code need integration tests? Does this CR introduce the risk of breaking test code, staging stacks, or integrations tests? Does this change break backward compatibility? Is it OK to merge the change at this point or should it be pushed into a later release? Was the external documentation updated?  Tips in Comments: concise, friendly, actionable  Written in neutral language Critique the code, not the author. Avoid possessive pronouns Avoid absolute judgements Try to differentiate between:  Suggestions (e.g., “Suggestion: extract method to improve legibility”) Required changes (e.g., “Add @Override”) Points that need discussion or clarification (e.g., “Is this really the correct behavior? If so, please add a comment explaining the logic.”)    Examples class MyClass { private int countTotalPageVisits; //R: name variables consistently  private int uniqueUsersCount; } interface MyInterface { /** Returns {@link Optional#empty} if s cannot be extracted. */ public Optional\u0026lt;String\u0026gt; extractString(String s); /** Returns null if {@code s} cannot be rewritten. */ //R: should harmonize return values: use Optional\u0026lt;\u0026gt; here, too  public String rewriteString(String s); } //R: remove and replace by Guava\u0026#39;s MapJoiner String joinAndConcatenate(Map\u0026lt;String, String\u0026gt; map, String keyValueSeparator, String keySeparator); int dayCount; //R: nit: I usually prefer numFoo over fooCount; up to you, but we should keep it consistent in this project //R: This performs numIterations+1 iterations, is that intentional? // If it is, consider changing the numIterations semantics? for (int i = 0; i \u0026lt;= numIterations; ++i) { ... } otherService.call(); //R: I think we should avoid the dependency on OtherService. Can we discuss this in person? "
},
{
	"uri": "https://ivangrod.github.io/se-journey/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ivangrod.github.io/se-journey/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]